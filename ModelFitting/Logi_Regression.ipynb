{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking the AAPL for example to investigate the effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.79374263e+00, -1.18868843e-01,  4.01859581e-02, ...,\n",
       "         5.05057466e+01,  1.40579557e+23,  9.48200377e+06],\n",
       "       [ 4.18722538e+00, -5.36824720e-02, -7.59893880e-02, ...,\n",
       "         5.92613609e+01,  1.37823095e+23,  1.82290811e+07],\n",
       "       [ 3.94897208e+00,  1.74515808e-01,  1.22371094e-01, ...,\n",
       "         6.66248645e+01,  1.35120681e+23,  2.72652181e+07],\n",
       "       ...,\n",
       "       [ 8.49680526e+00,  9.33175468e-01, -6.63043092e-02, ...,\n",
       "         0.00000000e+00,  2.97160000e+02,  1.75108076e+11],\n",
       "       [ 1.29197633e+01,  4.08612913e-01,  4.67014344e-02, ...,\n",
       "         0.00000000e+00,  3.04880000e+02,  1.77411455e+11],\n",
       "       [ 1.03502457e+01, -6.48851599e-01,  3.30228381e-01, ...,\n",
       "         0.00000000e+00,  3.04880000e+02,  1.75079001e+11]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = 'Daily'\n",
    "stock = 'AAPL'\n",
    "price = pd.read_csv('../encode_price/'+freq+'/'+stock+'.csv')\n",
    "y = price.direction.shift(-1).values[:-1]\n",
    "predictors = pd.read_csv('../predictors/Merged/'+freq+'/'+stock+'.csv',index_col='Date')\n",
    "predictors.fillna(0,inplace=True)\n",
    "predictors = predictors.drop(['gdp','adjusted_close'],axis=1)\n",
    "X = predictors.values[:-1,]\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,shuffle=False)\n",
    "sm = SMOTE(sampling_strategy='minority',random_state=42,k_neighbors=5)\n",
    "X_res,y_res = sm.fit_resample(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.59      0.57       288\n",
      "         1.0       0.64      0.62      0.63       346\n",
      "\n",
      "    accuracy                           0.61       634\n",
      "   macro avg       0.60      0.60      0.60       634\n",
      "weighted avg       0.61      0.61      0.61       634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(StandardScaler(),LogisticRegression())\n",
    "pipe.fit(X_res,y_res)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.56      0.55       288\n",
      "         1.0       0.63      0.61      0.62       346\n",
      "\n",
      "    accuracy                           0.59       634\n",
      "   macro avg       0.59      0.59      0.59       634\n",
      "weighted avg       0.59      0.59      0.59       634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(StandardScaler(),LogisticRegression())\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5252365930599369"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.kernel_approximation import PolynomialCountSketch\n",
    "# Approximates feature map of a Polynomial kernel by approximation via Tensor Sketch.\n",
    "poly_feature = PolynomialCountSketch(degree=1, random_state=2)\n",
    "X_features = poly_feature.fit_transform(X_train)\n",
    "X_features_t = poly_feature.fit_transform(X_test)\n",
    "#X_test = poly_feature.fit_transform(X_test)\n",
    "# Fit a Logistic Regression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_features, y_train)\n",
    "\n",
    "# Evaluate the KLR in-sample\n",
    "clf.score(X_features, y_train)\n",
    "#y_pred = clf.predict(X_features_t)\n",
    "#print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5935856992639327"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.kernel_approximation import RBFSampler\n",
    "rbf_feature = RBFSampler(gamma=1, random_state=123)\n",
    "X_features = rbf_feature.fit_transform(X_train)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_features, y_train)\n",
    "clf.score(X_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to X in AdditiveChi2Sampler.fit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/DELL/Duke/STA561/project/ModelFitting/Logi_Regression.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/DELL/Duke/STA561/project/ModelFitting/Logi_Regression.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkernel_approximation\u001b[39;00m \u001b[39mimport\u001b[39;00m PolynomialCountSketch,AdditiveChi2Sampler\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/DELL/Duke/STA561/project/ModelFitting/Logi_Regression.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m adchi2 \u001b[39m=\u001b[39m AdditiveChi2Sampler(sample_steps\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/DELL/Duke/STA561/project/ModelFitting/Logi_Regression.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m X_features \u001b[39m=\u001b[39m adchi2\u001b[39m.\u001b[39;49mfit_transform(X_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/DELL/Duke/STA561/project/ModelFitting/Logi_Regression.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m clf \u001b[39m=\u001b[39m LogisticRegression()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/DELL/Duke/STA561/project/ModelFitting/Logi_Regression.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m clf\u001b[39m.\u001b[39mfit(X_features, y_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    877\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m    879\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    880\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/kernel_approximation.py:681\u001b[0m, in \u001b[0;36mAdditiveChi2Sampler.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m    680\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(X, accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 681\u001b[0m check_non_negative(X, \u001b[39m\"\u001b[39;49m\u001b[39mX in AdditiveChi2Sampler.fit\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_interval \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    684\u001b[0m     \u001b[39m# See reference, figure 2 c)\u001b[39;00m\n\u001b[1;32m    685\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_steps \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1418\u001b[0m, in \u001b[0;36mcheck_non_negative\u001b[0;34m(X, whom)\u001b[0m\n\u001b[1;32m   1415\u001b[0m     X_min \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mmin(X)\n\u001b[1;32m   1417\u001b[0m \u001b[39mif\u001b[39;00m X_min \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1418\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNegative values in data passed to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m whom)\n",
      "\u001b[0;31mValueError\u001b[0m: Negative values in data passed to X in AdditiveChi2Sampler.fit"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_approximation import PolynomialCountSketch,AdditiveChi2Sampler\n",
    "adchi2 = AdditiveChi2Sampler(sample_steps=2)\n",
    "X_features = adchi2.fit_transform(X_train)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_features, y_train)\n",
    "clf.score(X_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.58      0.55       288\n",
      "         1.0       0.62      0.58      0.60       346\n",
      "\n",
      "    accuracy                           0.58       634\n",
      "   macro avg       0.58      0.58      0.58       634\n",
      "weighted avg       0.58      0.58      0.58       634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fit LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the LDA in-sample\n",
    "lda.score(X_train, y_train)\n",
    "y_pred = lda.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47791798107255523"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "# Fit QDA\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the QDA in-sample\n",
    "qda.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5ca7f1e19f63aad61e7f105267048d1a47f29c947be0f4c6ffca5d2ac1d455d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
