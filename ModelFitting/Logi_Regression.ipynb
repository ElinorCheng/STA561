{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking the AAPL for example to investigate the effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "freq = 'Daily'\n",
    "stock = 'AAPL'\n",
    "price = pd.read_csv('../encode_price/'+freq+'/'+stock+'.csv')\n",
    "y = price.direction.shift(-1).values[:-1]\n",
    "predictors = pd.read_csv('../predictors/Merged/'+freq+'/'+stock+'.csv',index_col='Date')\n",
    "predictors.fillna(0,inplace=True)\n",
    "#predictors = predictors.drop(['gdp','adjusted_close'],axis=1)\n",
    "X = predictors.values[:-1,]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,shuffle=False)\n",
    "sm = SMOTE(sampling_strategy='minority',random_state=42,k_neighbors=5)\n",
    "X_res,y_res = sm.fit_resample(X_train,y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SMOTE Balanced Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(),LogisticRegression())\n",
    "pipe.fit(X_res,y_res)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Original Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(),LogisticRegression())\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.kernel_approximation import PolynomialCountSketch\n",
    "# Approximates feature map of a Polynomial kernel by approximation via Tensor Sketch.\n",
    "poly_feature = PolynomialCountSketch(degree=1, random_state=2)\n",
    "X_features = poly_feature.fit_transform(X_train)\n",
    "X_features_t = poly_feature.fit_transform(X_test)\n",
    "#X_test = poly_feature.fit_transform(X_test)\n",
    "# Fit a Logistic Regression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_features, y_train)\n",
    "\n",
    "# Evaluate the KLR in-sample\n",
    "clf.score(X_features, y_train)\n",
    "#y_pred = clf.predict(X_features_t)\n",
    "#print(classification_report(y_test,y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6035751840168244"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.kernel_approximation import RBFSampler\n",
    "rbf_feature = RBFSampler(gamma=1, random_state=123)\n",
    "X_features = rbf_feature.fit_transform(X_train)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_features, y_train)\n",
    "clf.score(X_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to X in AdditiveChi2Sampler.fit",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m/Users/DELL/Duke/STA561/project/ModelFitting/Logi_Regression.ipynb Cell 13\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/DELL/Duke/STA561/project/ModelFitting/Logi_Regression.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39msklearn\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mkernel_approximation\u001B[39;00m \u001B[39mimport\u001B[39;00m PolynomialCountSketch,AdditiveChi2Sampler\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/DELL/Duke/STA561/project/ModelFitting/Logi_Regression.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001B[0m adchi2 \u001B[39m=\u001B[39m AdditiveChi2Sampler(sample_steps\u001B[39m=\u001B[39m\u001B[39m2\u001B[39m)\n\u001B[0;32m----> <a href='vscode-notebook-cell:/Users/DELL/Duke/STA561/project/ModelFitting/Logi_Regression.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001B[0m X_features \u001B[39m=\u001B[39m adchi2\u001B[39m.\u001B[39;49mfit_transform(X_train)\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/DELL/Duke/STA561/project/ModelFitting/Logi_Regression.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001B[0m clf \u001B[39m=\u001B[39m LogisticRegression()\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/DELL/Duke/STA561/project/ModelFitting/Logi_Regression.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001B[0m clf\u001B[39m.\u001B[39mfit(X_features, y_train)\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/_set_output.py:140\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m    138\u001B[0m \u001B[39m@wraps\u001B[39m(f)\n\u001B[1;32m    139\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mwrapped\u001B[39m(\u001B[39mself\u001B[39m, X, \u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs):\n\u001B[0;32m--> 140\u001B[0m     data_to_wrap \u001B[39m=\u001B[39m f(\u001B[39mself\u001B[39;49m, X, \u001B[39m*\u001B[39;49margs, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mkwargs)\n\u001B[1;32m    141\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39misinstance\u001B[39m(data_to_wrap, \u001B[39mtuple\u001B[39m):\n\u001B[1;32m    142\u001B[0m         \u001B[39m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[1;32m    143\u001B[0m         \u001B[39mreturn\u001B[39;00m (\n\u001B[1;32m    144\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[39m0\u001B[39m], X, \u001B[39mself\u001B[39m),\n\u001B[1;32m    145\u001B[0m             \u001B[39m*\u001B[39mdata_to_wrap[\u001B[39m1\u001B[39m:],\n\u001B[1;32m    146\u001B[0m         )\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:878\u001B[0m, in \u001B[0;36mTransformerMixin.fit_transform\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m    874\u001B[0m \u001B[39m# non-optimized default implementation; override when a better\u001B[39;00m\n\u001B[1;32m    875\u001B[0m \u001B[39m# method is possible for a given clustering algorithm\u001B[39;00m\n\u001B[1;32m    876\u001B[0m \u001B[39mif\u001B[39;00m y \u001B[39mis\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[1;32m    877\u001B[0m     \u001B[39m# fit method of arity 1 (unsupervised transformation)\u001B[39;00m\n\u001B[0;32m--> 878\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mfit(X, \u001B[39m*\u001B[39;49m\u001B[39m*\u001B[39;49mfit_params)\u001B[39m.\u001B[39mtransform(X)\n\u001B[1;32m    879\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m    880\u001B[0m     \u001B[39m# fit method of arity 2 (supervised transformation)\u001B[39;00m\n\u001B[1;32m    881\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mfit(X, y, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mfit_params)\u001B[39m.\u001B[39mtransform(X)\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/kernel_approximation.py:681\u001B[0m, in \u001B[0;36mAdditiveChi2Sampler.fit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    678\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_validate_params()\n\u001B[1;32m    680\u001B[0m X \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_validate_data(X, accept_sparse\u001B[39m=\u001B[39m\u001B[39m\"\u001B[39m\u001B[39mcsr\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[0;32m--> 681\u001B[0m check_non_negative(X, \u001B[39m\"\u001B[39;49m\u001B[39mX in AdditiveChi2Sampler.fit\u001B[39;49m\u001B[39m\"\u001B[39;49m)\n\u001B[1;32m    683\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39msample_interval \u001B[39mis\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[1;32m    684\u001B[0m     \u001B[39m# See reference, figure 2 c)\u001B[39;00m\n\u001B[1;32m    685\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39msample_steps \u001B[39m==\u001B[39m \u001B[39m1\u001B[39m:\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1418\u001B[0m, in \u001B[0;36mcheck_non_negative\u001B[0;34m(X, whom)\u001B[0m\n\u001B[1;32m   1415\u001B[0m     X_min \u001B[39m=\u001B[39m xp\u001B[39m.\u001B[39mmin(X)\n\u001B[1;32m   1417\u001B[0m \u001B[39mif\u001B[39;00m X_min \u001B[39m<\u001B[39m \u001B[39m0\u001B[39m:\n\u001B[0;32m-> 1418\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mValueError\u001B[39;00m(\u001B[39m\"\u001B[39m\u001B[39mNegative values in data passed to \u001B[39m\u001B[39m%s\u001B[39;00m\u001B[39m\"\u001B[39m \u001B[39m%\u001B[39m whom)\n",
      "\u001B[0;31mValueError\u001B[0m: Negative values in data passed to X in AdditiveChi2Sampler.fit"
     ]
    }
   ],
   "source": [
    "from sklearn.kernel_approximation import PolynomialCountSketch,AdditiveChi2Sampler\n",
    "adchi2 = AdditiveChi2Sampler(sample_steps=2)\n",
    "X_features = adchi2.fit_transform(X_train)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_features, y_train)\n",
    "clf.score(X_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.58      0.55       288\n",
      "         1.0       0.62      0.58      0.60       346\n",
      "\n",
      "    accuracy                           0.58       634\n",
      "   macro avg       0.58      0.58      0.58       634\n",
      "weighted avg       0.58      0.58      0.58       634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fit LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the LDA in-sample\n",
    "lda.score(X_train, y_train)\n",
    "y_pred = lda.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45425867507886436"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "# Fit QDA\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the QDA in-sample\n",
    "qda.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5ca7f1e19f63aad61e7f105267048d1a47f29c947be0f4c6ffca5d2ac1d455d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}