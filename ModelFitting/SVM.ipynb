{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix,classification_report,f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Direction as the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking AAPL Daily stock data as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 'Daily'\n",
    "stock = 'AAPL'\n",
    "\n",
    "price = pd.read_csv('../encode_price/'+freq+'/'+stock+'.csv')\n",
    "price = price.loc[(price.Date<='2019-12-31')&(price.Date>='2010-01-04'),:]\n",
    "price = price.loc[(price.Date>='2010-01-04'),:]\n",
    "y = price.direction.shift(-1).values[:-1]\n",
    "predictors = pd.read_csv('../predictors/Merged/'+freq+'/'+stock+'.csv')\n",
    "NLP = pd.read_csv('../predictors/NLP/'+freq+'/NYT_macro_SA.csv')\n",
    "predictors = pd.merge(predictors,NLP,how='left',on=['Date'])\n",
    "predictors = predictors.loc[predictors.Date <= '2019-12-31',:]\n",
    "predictors.set_index('Date',inplace=True)\n",
    "predictors.fillna(0,inplace=True) \n",
    "X = predictors.values[:-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.02      0.03       227\n",
      "         1.0       0.55      0.99      0.71       276\n",
      "\n",
      "    accuracy                           0.55       503\n",
      "   macro avg       0.61      0.51      0.37       503\n",
      "weighted avg       0.60      0.55      0.40       503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,shuffle=False)\n",
    "sm = SMOTE(sampling_strategy='minority',random_state=42,k_neighbors=5)\n",
    "X_res,y_res = sm.fit_resample(X_train,y_train)\n",
    "pipe = make_pipeline(StandardScaler(),SVC(kernel='linear'))\n",
    "pipe.fit(X_res,y_res)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.97      0.70       341\n",
      "         1.0       0.93      0.34      0.50       414\n",
      "\n",
      "    accuracy                           0.62       755\n",
      "   macro avg       0.74      0.65      0.60       755\n",
      "weighted avg       0.76      0.62      0.59       755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,shuffle=False)\n",
    "pipe = make_pipeline(StandardScaler(),SVC(kernel='poly',degree=5))\n",
    "pipe.fit(X_res,y_res)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.23      0.36       341\n",
      "         1.0       0.60      0.93      0.73       414\n",
      "\n",
      "    accuracy                           0.62       755\n",
      "   macro avg       0.67      0.58      0.54       755\n",
      "weighted avg       0.67      0.62      0.56       755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,shuffle=False)\n",
    "pipe = make_pipeline(StandardScaler(),SVC(kernel='rbf'))\n",
    "pipe.fit(X_res,y_res)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.42      0.51      0.46       341\n",
      "         1.0       0.50      0.41      0.45       414\n",
      "\n",
      "    accuracy                           0.45       755\n",
      "   macro avg       0.46      0.46      0.45       755\n",
      "weighted avg       0.46      0.45      0.45       755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,shuffle=False)\n",
    "pipe = make_pipeline(StandardScaler(),SVC(kernel='sigmoid'))\n",
    "pipe.fit(X_res,y_res)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Direction2 to be the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "freq = 'Daily'\n",
    "stock = 'AAPL'\n",
    "\n",
    "price = pd.read_csv('../encode_price/'+freq+'/'+stock+'.csv')\n",
    "price = price.loc[(price.Date<='2019-12-31')&(price.Date>='2010-01-04'),:]\n",
    "price = price.loc[(price.Date>='2010-01-04'),:]\n",
    "y = price.direction2.shift(-1).values[:-1]\n",
    "predictors = pd.read_csv('../predictors/Merged/'+freq+'/'+stock+'.csv')\n",
    "NLP = pd.read_csv('../predictors/NLP/'+freq+'/NYT_macro_SA.csv')\n",
    "predictors = pd.merge(predictors,NLP,how='left',on=['Date'])\n",
    "predictors = predictors.loc[predictors.Date <= '2019-12-31',:]\n",
    "predictors.set_index('Date',inplace=True)\n",
    "predictors.fillna(0,inplace=True) \n",
    "X = predictors.values[:-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.31      0.88      0.46       164\n",
      "         1.0       0.55      0.07      0.12       339\n",
      "\n",
      "    accuracy                           0.33       503\n",
      "   macro avg       0.43      0.48      0.29       503\n",
      "weighted avg       0.47      0.33      0.23       503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,shuffle=False)\n",
    "sm = SMOTE(sampling_strategy='minority',random_state=42,k_neighbors=5)\n",
    "X_res,y_res = sm.fit_resample(X_train,y_train)\n",
    "pipe = make_pipeline(StandardScaler(),SVC(kernel='linear'))\n",
    "pipe.fit(X_res,y_res)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.01      0.02       164\n",
      "         1.0       0.68      1.00      0.81       339\n",
      "\n",
      "    accuracy                           0.68       503\n",
      "   macro avg       0.84      0.51      0.42       503\n",
      "weighted avg       0.78      0.68      0.55       503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,shuffle=False)\n",
    "pipe = make_pipeline(StandardScaler(),SVC(kernel='poly',degree=5))\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.12      0.21       232\n",
      "         1.0       0.72      1.00      0.84       523\n",
      "\n",
      "    accuracy                           0.73       755\n",
      "   macro avg       0.86      0.56      0.52       755\n",
      "weighted avg       0.80      0.73      0.64       755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,shuffle=False)\n",
    "pipe = make_pipeline(StandardScaler(),SVC(kernel='rbf'))\n",
    "pipe.fit(X_res,y_res)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.29      0.56      0.38       164\n",
      "         1.0       0.62      0.34      0.44       339\n",
      "\n",
      "    accuracy                           0.41       503\n",
      "   macro avg       0.45      0.45      0.41       503\n",
      "weighted avg       0.51      0.41      0.42       503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,shuffle=False)\n",
    "pipe = make_pipeline(StandardScaler(),SVC(kernel= 'sigmoid'))\n",
    "pipe.fit(X_res,y_res)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We found that direction 2 is super powerful based on the result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above result, we can find that RBF kernel and polynomial kernel are super powerful. So, we will select rbf kernel for the future tuning purpose to save the complexity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection Task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the part, we will use the weighted F1-score to evaluate the classification problem, which will include the biase situation into the consideration, which will place a penalty on the biased prediction result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_list = ['SMA','EMA','STOCH_k','STOCK_d','RSI','MFI','SAR','AD','MACD','MACD_Signal','MACD_Histo','VWAP','SPY','NDAQ','PC1','PC2']\n",
    "funda_list = ['pcf','PEG_trailing','dpr','npm','gpm','roa','roe','capital_ratio','de_ratio','cash_ratio','curr_ratio','inv_turn','pay_turn','sale_nwc','rd_sale','accrual']\n",
    "macro_list = ['gdpr1','gdpr2','cpi','bond20yr','bond30yr','fedfunds','cpir','wpir','unemp','employ']\n",
    "nlp_list = ['Pos_lag2','Pos_lag3','Neg_lag1','Neg_lag2','Neg_lag3','Neu_lag1','Neu_lag2','Neu_lag3']\n",
    "stock_list = ['AAPL','AMZN','BRK-B','GOOG','JNJ','META','MSFT','NVDA','TSLA','V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_score = dict()\n",
    "for j in stock_list:\n",
    "    freq = 'Daily'\n",
    "    stock = j\n",
    "    price = pd.read_csv('../encode_price/'+freq+'/'+stock+'.csv')\n",
    "    price = price.loc[(price.Date<='2019-12-31')&(price.Date>='2010-01-04'),:]\n",
    "    price = price.loc[(price.Date>='2010-01-04'),:]\n",
    "    y = price.direction2.shift(-1).values[:-1]\n",
    "    predictors = pd.read_csv('../predictors/Merged/'+freq+'/'+stock+'.csv')\n",
    "    NLP = pd.read_csv('../predictors/NLP/Daily/NYT_macro_SA.csv')\n",
    "    predictors = pd.merge(predictors,NLP,how='left',on=['Date'])\n",
    "    predictors = predictors.loc[predictors.Date <= '2019-12-31',:]\n",
    "    predictors.set_index('Date',inplace=True)\n",
    "    predictors.fillna(0,inplace=True) \n",
    "    X = predictors.values[:-1]\n",
    "    cv = 3\n",
    "    scores = []\n",
    "    tscv = TimeSeriesSplit(n_splits=cv)\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        pipe = make_pipeline(StandardScaler(),SVC(kernel='rbf'))\n",
    "        sfs = SequentialFeatureSelector(pipe,n_jobs = -1,n_features_to_select='auto',scoring='f1_weighted')\n",
    "        sfs.fit(X_train,y_train)\n",
    "        X_train = sfs.transform(X_train)\n",
    "        pipe.fit(X_train,y_train)\n",
    "        X_test =  sfs.transform(X_test)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "        scores.append(f1_score(y_pred,y_test,average = 'weighted'))\n",
    "    average_score = np.mean(scores)\n",
    "    print(j,average_score)\n",
    "    stock_score[j] = average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 5\n",
    "scores = dict()\n",
    "selection_collect = dict()\n",
    "for j in stock_list:\n",
    "    freq = 'Daily'\n",
    "    stock = j\n",
    "    price = pd.read_csv('../encode_price/'+freq+'/'+stock+'.csv')\n",
    "    price = price.loc[(price.Date<='2019-12-31')&(price.Date>='2010-01-04'),:]\n",
    "    price = price.loc[(price.Date>='2010-01-04'),:]\n",
    "    y = price.adjusted_close.shift(-1).values[:-1]\n",
    "    predictors = pd.read_csv('../predictors/Merged/'+freq+'/'+stock+'.csv')\n",
    "    NLP = pd.read_csv('../predictors/NLP/Daily/NYT_macro_SA.csv')\n",
    "    predictors = pd.merge(predictors,NLP,how='left',on=['Date'])\n",
    "    predictors = predictors.loc[predictors.Date <= '2019-12-31',:]\n",
    "    predictors.set_index('Date',inplace=True)\n",
    "    predictors.fillna(0,inplace=True) \n",
    "    X = predictors.values[:-1]\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=cv)\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,shuffle=False)\n",
    "    pipe = make_pipeline(StandardScaler(),SVC(kernel='poly',degree=5))\n",
    "    \n",
    "    sfs = SequentialFeatureSelector(pipe,n_jobs = -1,n_features_to_select='auto',scoring='f1_weighted',cv=tscv)\n",
    "    sfs.fit(X_train,y_train)\n",
    "    X_train = sfs.transform(X_train)\n",
    "    pipe.fit(X_train,y_train)\n",
    "    selection_collect[j] = predictors.columns.values[sfs.get_support()]\n",
    "    X_test =  sfs.transform(X_test)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    scores[j] = f1_score(y_pred,y_test,average = 'weighted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5ca7f1e19f63aad61e7f105267048d1a47f29c947be0f4c6ffca5d2ac1d455d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
