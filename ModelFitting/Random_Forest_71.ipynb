{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix,classification_report,f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Direction as the dependent variable\n",
    "### Taking AAPL Daily stock data as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 'Daily'\n",
    "stock = 'AAPL'\n",
    "\n",
    "price = pd.read_csv('../encode_price/'+freq+'/'+stock+'.csv')\n",
    "price = price.loc[(price.Date<='2019-12-31')&(price.Date>='2010-01-04'),:]\n",
    "price = price.loc[(price.Date>='2010-01-04'),:]\n",
    "y = price.direction.shift(-1).values[:-1]\n",
    "predictors = pd.read_csv('../predictors/Merged/'+freq+'/'+stock+'.csv')\n",
    "NLP = pd.read_csv('../predictors/NLP/'+freq+'/NYT_macro_SA.csv')\n",
    "predictors = pd.merge(predictors,NLP,how='left',on=['Date'])\n",
    "predictors = predictors.loc[predictors.Date <= '2019-12-31',:]\n",
    "predictors.set_index('Date',inplace=True)\n",
    "predictors.fillna(0,inplace=True) \n",
    "X = predictors.values[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      0.72      0.56       341\n",
      "         1.0       0.56      0.29      0.38       414\n",
      "\n",
      "    accuracy                           0.48       755\n",
      "   macro avg       0.51      0.51      0.47       755\n",
      "weighted avg       0.51      0.48      0.46       755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,shuffle=False)\n",
    "sm = SMOTE(sampling_strategy='minority',random_state=42,k_neighbors=4)\n",
    "X_res,y_res = sm.fit_resample(X_train,y_train)\n",
    "pipe = make_pipeline(MinMaxScaler(),RandomForestClassifier(criterion='entropy',max_depth = 100,max_features = 'sqrt',class_weight='balanced_subsample'))\n",
    "pipe.fit(X_res,y_res)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      0.71      0.56       341\n",
      "         1.0       0.57      0.32      0.41       414\n",
      "\n",
      "    accuracy                           0.50       755\n",
      "   macro avg       0.52      0.52      0.49       755\n",
      "weighted avg       0.52      0.50      0.48       755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,shuffle=False)\n",
    "pipe = make_pipeline(StandardScaler(),XGBClassifier(base_score=0.5, booster='gbtree',objective='binary:logistic', learning_rate=0.1, max_depth=5,n_estimators=100))\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Direction2 to be the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 'Daily'\n",
    "stock = 'AAPL'\n",
    "\n",
    "price = pd.read_csv('../encode_price/'+freq+'/'+stock+'.csv')\n",
    "price = price.loc[(price.Date<='2019-12-31')&(price.Date>='2010-01-04'),:]\n",
    "price = price.loc[(price.Date>='2010-01-04'),:]\n",
    "y = price.direction2.shift(-1).values[:-1]\n",
    "predictors = pd.read_csv('../predictors/Merged/'+freq+'/'+stock+'.csv')\n",
    "NLP = pd.read_csv('../predictors/NLP/'+freq+'/NYT_macro_SA.csv')\n",
    "predictors = pd.merge(predictors,NLP,how='left',on=['Date'])\n",
    "predictors = predictors.loc[predictors.Date <= '2019-12-31',:]\n",
    "predictors.set_index('Date',inplace=True)\n",
    "predictors.fillna(0,inplace=True) \n",
    "X = predictors.values[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.21      0.27       164\n",
      "         1.0       0.69      0.85      0.76       339\n",
      "\n",
      "    accuracy                           0.64       503\n",
      "   macro avg       0.54      0.53      0.52       503\n",
      "weighted avg       0.59      0.64      0.60       503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,shuffle=False)\n",
    "sm = SMOTE(sampling_strategy='minority',random_state=42,k_neighbors=4)\n",
    "X_res,y_res = sm.fit_resample(X_train,y_train)\n",
    "pipe = make_pipeline(MinMaxScaler(),RandomForestClassifier(criterion='entropy',max_depth = 100,max_features = 'sqrt',class_weight='balanced_subsample'))\n",
    "pipe.fit(X_res,y_res)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.31      0.21      0.25       232\n",
      "         1.0       0.69      0.80      0.74       523\n",
      "\n",
      "    accuracy                           0.62       755\n",
      "   macro avg       0.50      0.50      0.50       755\n",
      "weighted avg       0.58      0.62      0.59       755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,shuffle=False)\n",
    "pipe = make_pipeline(MinMaxScaler(),XGBClassifier(base_score=0.5, booster='gbtree',objective='binary:logistic', learning_rate=0.1, max_depth=5,n_estimators=1000))\n",
    "pipe.fit(X_train,y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above results we can see Random Forest and Xgboost do not perform well both on direction 1 and direction 2. But we still want to try if feature selection can help improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_list = ['SMA','EMA','STOCH_k','STOCK_d','RSI','MFI','SAR','AD','MACD','MACD_Signal','MACD_Histo','VWAP','SPY','NDAQ','PC1','PC2']\n",
    "funda_list = ['pcf','PEG_trailing','dpr','npm','gpm','roa','roe','capital_ratio','de_ratio','cash_ratio','curr_ratio','inv_turn','pay_turn','sale_nwc','rd_sale','accrual']\n",
    "macro_list = ['gdpr1','gdpr2','cpi','bond20yr','bond30yr','fedfunds','cpir','wpir','unemp','employ']\n",
    "nlp_list = ['Pos_lag2','Pos_lag3','Neg_lag1','Neg_lag2','Neg_lag3','Neu_lag1','Neu_lag2','Neu_lag3']\n",
    "stock_list = ['AAPL','AMZN','BRK-B','GOOG','JNJ','META','MSFT','NVDA','TSLA','V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL 0.5713243522279255\n",
      "AMZN 0.5444843373331719\n",
      "BRK-B 0.49905601039933295\n",
      "GOOG 0.5022725157181038\n",
      "JNJ 0.5610963985000538\n",
      "META 0.5990741680534032\n",
      "MSFT 0.5418901716291072\n",
      "NVDA 0.49386992571793\n",
      "TSLA 0.5659073753244585\n",
      "V 0.5131946143020236\n"
     ]
    }
   ],
   "source": [
    "stock_score = dict()\n",
    "for j in stock_list:\n",
    "    freq = 'Daily'\n",
    "    stock = j\n",
    "    price = pd.read_csv('../encode_price/'+freq+'/'+stock+'.csv')\n",
    "    price = price.loc[(price.Date<='2019-12-31')&(price.Date>='2010-01-04'),:]\n",
    "    price = price.loc[(price.Date>='2010-01-04'),:]\n",
    "    y = price.direction2.shift(-1).values[:-1]\n",
    "    predictors = pd.read_csv('../predictors/Merged/'+freq+'/'+stock+'.csv')\n",
    "    NLP = pd.read_csv('../predictors/NLP/Daily/NYT_macro_SA.csv')\n",
    "    predictors = pd.merge(predictors,NLP,how='left',on=['Date'])\n",
    "    predictors = predictors.loc[predictors.Date <= '2019-12-31',:]\n",
    "    predictors.set_index('Date',inplace=True)\n",
    "    predictors.fillna(0,inplace=True) \n",
    "    X = predictors.values[:-1]\n",
    "    cv = 3\n",
    "    scores = []\n",
    "    tscv = TimeSeriesSplit(n_splits=cv)\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        pipe = make_pipeline(MinMaxScaler(),RandomForestClassifier(criterion='log_loss'))\n",
    "        sfs = SequentialFeatureSelector(pipe,n_jobs = -1,n_features_to_select=5,scoring='f1_weighted')\n",
    "        sfs.fit(X_train,y_train)\n",
    "        X_train = sfs.transform(X_train)\n",
    "        pipe.fit(X_train,y_train)\n",
    "        X_test =  sfs.transform(X_test)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "        scores.append(f1_score(y_pred,y_test,average = 'weighted'))\n",
    "    average_score = np.mean(scores)\n",
    "    print(j,average_score)\n",
    "    stock_score[j] = average_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL 0.5824460888713481\n",
      "AMZN 0.5339341885693225\n",
      "BRK-B 0.5884912030987596\n",
      "GOOG 0.5714190219889693\n",
      "JNJ 0.5393843060877742\n",
      "META 0.5191499644725684\n",
      "MSFT 0.5683555826897859\n",
      "NVDA 0.5039365334042633\n",
      "TSLA 0.5536460028392852\n",
      "V 0.5169206615334808\n"
     ]
    }
   ],
   "source": [
    "stock_score = dict()\n",
    "for j in stock_list:\n",
    "    freq = 'Daily'\n",
    "    stock = j\n",
    "    price = pd.read_csv('../encode_price/'+freq+'/'+stock+'.csv')\n",
    "    price = price.loc[(price.Date<='2019-12-31')&(price.Date>='2010-01-04'),:]\n",
    "    price = price.loc[(price.Date>='2010-01-04'),:]\n",
    "    y = price.direction2.shift(-1).values[:-1]\n",
    "    predictors = pd.read_csv('../predictors/Merged/'+freq+'/'+stock+'.csv')\n",
    "    NLP = pd.read_csv('../predictors/NLP/Daily/NYT_macro_SA.csv')\n",
    "    predictors = pd.merge(predictors,NLP,how='left',on=['Date'])\n",
    "    predictors = predictors.loc[predictors.Date <= '2019-12-31',:]\n",
    "    predictors.set_index('Date',inplace=True)\n",
    "    predictors.fillna(0,inplace=True) \n",
    "    X = predictors.values[:-1]\n",
    "    cv = 3\n",
    "    scores = []\n",
    "    tscv = TimeSeriesSplit(n_splits=cv)\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        pipe = make_pipeline(MinMaxScaler(),XGBClassifier(base_score=0.5, booster='gbtree',objective='binary:logistic', learning_rate=0.1, max_depth=5,n_estimators=100))\n",
    "        sfs = SequentialFeatureSelector(pipe,n_jobs = -1,n_features_to_select=5,scoring='f1_weighted')\n",
    "        sfs.fit(X_train,y_train)\n",
    "        X_train = sfs.transform(X_train)\n",
    "        pipe.fit(X_train,y_train)\n",
    "        X_test =  sfs.transform(X_test)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "        scores.append(f1_score(y_pred,y_test,average = 'weighted'))\n",
    "    average_score = np.mean(scores)\n",
    "    print(j,average_score)\n",
    "    stock_score[j] = average_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparrently, feature selection brings limited improvements on the performance of Random Forest and Xgboost. For classification problem we may try other models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5ca7f1e19f63aad61e7f105267048d1a47f29c947be0f4c6ffca5d2ac1d455d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
